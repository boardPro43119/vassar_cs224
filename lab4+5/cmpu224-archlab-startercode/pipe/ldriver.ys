#######################################################################
# Test for copying block of size 63;
#######################################################################
	.pos 0
main:	irmovq Stack, %rsp  	# Set up stack pointer

	# Set up arguments for copy function and then invoke it
	irmovq $63, %rdx		# src and dst have 63 elements
	irmovq dest, %rsi	# dst array
	irmovq src, %rdi	# src array
	call ncopy		 
	halt			# should halt with num nonzeros in %rax
StartFun:
#/* $begin ncopy-ys */
##################################################################
# ncopy.ys - Copy a src block of len words to dst.
# Return the number of positive words (>0) contained in src.
#
# Benjamin Prud'homme
#
# Describe how and why you modified the baseline code.
# Replaced irmovq/addq with iaddq
# Unrolled loop to copy 8 elements at a time
# Reordered instructions to avoid bubbling/stalling (ex. put at least one
# instruction between mrmovq and next instruction involving that register)
#
##################################################################
# Do not modify this portion
# Function prologue.
# %rdi = src, %rsi = dst, %rdx = len
ncopy:

##################################################################
# You can modify this portion
	# Loop header
	xorq %rax,%rax		# count = 0;
	iaddq $-8, %rdx		# len < 8?
	jl Rem1			# if so, goto Rem1

Copy:	mrmovq (%rdi), %r8	# copy 8 elements to registers
	mrmovq 8(%rdi), %r9
	mrmovq 16(%rdi), %r10
	mrmovq 24(%rdi), %r11
	mrmovq 32(%rdi), %r12
	mrmovq 40(%rdi), %r13
	mrmovq 48(%rdi), %r14
	mrmovq 56(%rdi), %rbx

Copy1:	andq %r8, %r8		# element > 0?
	rmmovq %r8, (%rsi)	# copy element 1 to destination
	jle Copy2		
	iaddq $1, %rax		# if so, count++

Copy2:	andq %r9, %r9		# repeat for elements 2-8
	rmmovq %r9, 8(%rsi)	
	jle Copy3
	iaddq $1, %rax	

Copy3:	andq %r10, %r10
	rmmovq %r10, 16(%rsi)	
	jle Copy4
	iaddq $1, %rax

Copy4:	andq %r11, %r11
	rmmovq %r11, 24(%rsi)	
	jle Copy5
	iaddq $1, %rax

Copy5:	andq %r12, %r12
	rmmovq %r12, 32(%rsi)	
	jle Copy6
	iaddq $1, %rax

Copy6:	andq %r13, %r13
	rmmovq %r13, 40(%rsi)	
	jle Copy7
	iaddq $1, %rax

Copy7:	andq %r14, %r14
	rmmovq %r14, 48(%rsi)	
	jle Copy8
	iaddq $1, %rax

Copy8:	andq %rbx, %rbx
	rmmovq %rbx, 56(%rsi)	
	jle Npos
	iaddq $1, %rax

Npos:	iaddq $64, %rdi		# 0x191 src+=8
	iaddq $64, %rsi		# 0x19b dst+=8
	iaddq $-8, %rdx		# 0x1a5 len-=8
	jge Copy		# 0x1b9 if len >= 8, goto Copy8 (continue loop):
	

Rem1:	iaddq $8, %rdx		# store len in %rdx (previously stored len-8), set condition codes
	mrmovq (%rdi), %r8	# move remainder element 1 to register
	jle Done		# if len <=0, goto Done
	andq %r8, %r8		# element > 0?
	rmmovq %r8, (%rsi)	# copy element to memory
	jle Rem2		 
	iaddq $1, %rax		# if so, count++

Rem2:	iaddq $-1, %rdx	# 0x1c2 # decrement len, set condition codes
	mrmovq 8(%rdi), %r9	# repeat for remaining elements
	jle Done
	andq %r9, %r9	
	rmmovq %r9, 8(%rsi)
	jle Rem3
	iaddq $1, %rax	

Rem3:	iaddq $-1, %rdx		
	mrmovq 16(%rdi), %r10
	jle Done
	andq %r10, %r10
	rmmovq %r10, 16(%rsi)
	jle Rem4
	iaddq $1, %rax

Rem4:	iaddq $-1, %rdx		
	mrmovq 24(%rdi), %r11
	jle Done
	andq %r11, %r11
	rmmovq %r11, 24(%rsi)
	jle Rem5
	iaddq $1, %rax

Rem5:	iaddq $-1, %rdx		
	mrmovq 32(%rdi), %r12
	jle Done
	andq %r12, %r12
	rmmovq %r12, 32(%rsi)
	jle Rem6
	iaddq $1, %rax

Rem6:	iaddq $-1, %rdx		
	mrmovq 40(%rdi), %r13
	jle Done
	andq %r13, %r13
	rmmovq %r13, 40(%rsi)
	jle Rem7
	iaddq $1, %rax

Rem7:	iaddq $-1, %rdx		
	mrmovq 48(%rdi), %r14
	jle Done
	andq %r14, %r14
	rmmovq %r14, 48(%rsi)
	jle Done
	iaddq $1, %rax

##################################################################
# Do not modify the following section of code
# Function epilogue.
Done:
	ret			
##################################################################

# Keep the following label at the end of your function
End:
#/* $end ncopy-ys */
EndFun:

###############################
# Source and destination blocks 
###############################
	.align 8
src:
	.quad 1
	.quad 2
	.quad -3
	.quad -4
	.quad -5
	.quad 6
	.quad 7
	.quad -8
	.quad 9
	.quad -10
	.quad -11
	.quad 12
	.quad -13
	.quad -14
	.quad 15
	.quad -16
	.quad -17
	.quad -18
	.quad -19
	.quad 20
	.quad -21
	.quad 22
	.quad 23
	.quad -24
	.quad 25
	.quad -26
	.quad 27
	.quad 28
	.quad 29
	.quad 30
	.quad 31
	.quad -32
	.quad -33
	.quad 34
	.quad 35
	.quad 36
	.quad -37
	.quad -38
	.quad 39
	.quad 40
	.quad -41
	.quad 42
	.quad -43
	.quad 44
	.quad -45
	.quad -46
	.quad 47
	.quad 48
	.quad 49
	.quad -50
	.quad 51
	.quad -52
	.quad 53
	.quad 54
	.quad 55
	.quad 56
	.quad -57
	.quad -58
	.quad -59
	.quad -60
	.quad -61
	.quad -62
	.quad -63
	.quad 0xbcdefa # This shouldn't get moved

	.align 16
Predest:
	.quad 0xbcdefa
dest:
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
Postdest:
	.quad 0xdefabc

.align 8
# Run time stack
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0

Stack:
